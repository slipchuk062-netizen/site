from fastapi import FastAPI, APIRouter, HTTPException, BackgroundTasks, Depends, Header, Request
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field, ConfigDict, EmailStr
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime, timezone
import httpx
import json
from emergentintegrations.llm.chat import LlmChat, UserMessage


ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection
mongo_url = os.environ['MONGO_URL']
client = AsyncIOMotorClient(mongo_url)
db = client[os.environ['DB_NAME']]

# Telegram configuration
TELEGRAM_BOT_TOKEN = os.environ.get('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHAT_ID = os.environ.get('TELEGRAM_CHAT_ID')

# LLM configuration
EMERGENT_LLM_KEY = os.environ.get('EMERGENT_LLM_KEY')

# Google Places API
GOOGLE_PLACES_API_KEY = os.environ.get('GOOGLE_PLACES_API_KEY')

# Admin password
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', 'admin123')

# Load attractions data for AI context
ATTRACTIONS_FILE = ROOT_DIR.parent / 'frontend' / 'src' / 'data' / 'attractions.json'
ATTRACTIONS_DATA = []
if ATTRACTIONS_FILE.exists():
    with open(ATTRACTIONS_FILE, 'r', encoding='utf-8') as f:
        ATTRACTIONS_DATA = json.load(f)

# Load districts data
DISTRICTS_FILE = ROOT_DIR.parent / 'frontend' / 'src' / 'data' / 'districts.js'

# ============= GEOPANDAS MODULE (–†–æ–∑–¥—ñ–ª 2.5) =============
# –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ –≥–µ–æ—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ GeoPandas —Ç–∞ Shapely
import geopandas as gpd
from shapely.geometry import Point, shape
import re

# –†–∞–π–æ–Ω–∏ –ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–æ—ó –æ–±–ª–∞—Å—Ç—ñ –∑ GeoJSON (–∑–≥—ñ–¥–Ω–æ –†–æ–∑–¥—ñ–ª—É 2.5)
DISTRICTS_GEODATA = None  # GeoDataFrame –¥–ª—è spatial join

def load_districts_geojson():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–µ–∂ —Ä–∞–π–æ–Ω—ñ–≤ –ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–æ—ó –æ–±–ª–∞—Å—Ç—ñ —è–∫ GeoDataFrame
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Å–∏—Å—Ç–µ–º—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç WGS84 (EPSG:4326) - –†–æ–∑–¥—ñ–ª 2.5
    """
    global DISTRICTS_GEODATA
    
    try:
        # –ü–∞—Ä—Å–∏–º–æ JavaScript —Ñ–∞–π–ª –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è GeoJSON –¥–∞–Ω–∏—Ö
        if DISTRICTS_FILE.exists():
            with open(DISTRICTS_FILE, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –í–∏—Ç—è–≥—É—î–º–æ –º–∞—Å–∏–≤ —Ä–∞–π–æ–Ω—ñ–≤ –∑ JavaScript
            # –®—É–∫–∞—î–º–æ bounds –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä–∞–π–æ–Ω—É
            districts_data = []
            
            # –†–µ–≥—É–ª—è—Ä–Ω–∏–π –≤–∏—Ä–∞–∑ –¥–ª—è –ø–æ—à—É–∫—É –æ–±'—î–∫—Ç—ñ–≤ —Ä–∞–π–æ–Ω—ñ–≤
            district_pattern = r'\{\s*id:\s*"([^"]+)".*?name:\s*"([^"]+)".*?bounds:\s*(\{.*?"type":\s*"Feature".*?\})\s*\}'
            
            # –°–ø—Ä–æ—â–µ–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥ - –≤–∏—Ç—è–≥—É—î–º–æ GeoJSON —á–∞—Å—Ç–∏–Ω–∏
            import re
            
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ Feature –æ–±'—î–∫—Ç–∏
            feature_pattern = r'"type":\s*"Feature".*?"geometry":\s*\{[^}]+\[[^\]]+\]\s*\}'
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ - –ø–∞—Ä—Å–∏–º–æ –≤—Ä—É—á–Ω—É
            districts_info = [
                {
                    "id": "zhytomyr",
                    "name": "–ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–∏–π —Ä–∞–π–æ–Ω",
                    "center": [50.2377, 28.6381],
                    "bounds": [[27.15, 49.75], [29.73, 50.75]]
                },
                {
                    "id": "berdychiv", 
                    "name": "–ë–µ—Ä–¥–∏—á—ñ–≤—Å—å–∫–∏–π —Ä–∞–π–æ–Ω",
                    "center": [49.8833, 28.6],
                    "bounds": [[28.0, 49.5], [29.0, 50.1]]
                },
                {
                    "id": "korosten",
                    "name": "–ö–æ—Ä–æ—Å—Ç–µ–Ω—Å—å–∫–∏–π —Ä–∞–π–æ–Ω", 
                    "center": [50.9553, 28.6494],
                    "bounds": [[27.5, 50.5], [29.0, 51.5]]
                },
                {
                    "id": "novograd",
                    "name": "–ù–æ–≤–æ–≥—Ä–∞–¥-–í–æ–ª–∏–Ω—Å—å–∫–∏–π —Ä–∞–π–æ–Ω",
                    "center": [50.5833, 27.6167],
                    "bounds": [[26.8, 50.0], [28.0, 51.0]]
                }
            ]
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ø—Ä–æ—â–µ–Ω—ñ –ø–æ–ª—ñ–≥–æ–Ω–∏ –¥–ª—è —Ä–∞–π–æ–Ω—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ bounds
            from shapely.geometry import box
            
            features = []
            for d in districts_info:
                bounds = d["bounds"]
                # box(minx, miny, maxx, maxy)
                polygon = box(bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1])
                features.append({
                    "id": d["id"],
                    "name": d["name"],
                    "center_lat": d["center"][0],
                    "center_lng": d["center"][1],
                    "geometry": polygon
                })
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ GeoDataFrame
            DISTRICTS_GEODATA = gpd.GeoDataFrame(features, crs="EPSG:4326")
            logger.info(f"Loaded {len(DISTRICTS_GEODATA)} districts into GeoDataFrame")
            return True
            
    except Exception as e:
        logger.error(f"Error loading districts GeoJSON: {str(e)}")
        return False
    
    return False


def determine_district_for_point(lat: float, lng: float) -> dict:
    """
    –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–∞–π–æ–Ω–Ω–æ—ó –ø—Ä–∏–Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ —Ç–æ—á–∫–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é spatial join (–†–æ–∑–¥—ñ–ª 2.5)
    
    GeoPandas –∑–∞–±–µ–∑–ø–µ—á—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π, –∑–æ–∫—Ä–µ–º–∞ 
    –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏–Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ —Ç–æ—á–∫–∏ –¥–æ –ø–æ–ª—ñ–≥–æ–Ω—É –¥–ª—è –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è 
    —Ä–∞–π–æ–Ω–Ω–æ—ó –ø—Ä–∏–Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –æ–±'—î–∫—Ç—ñ–≤.
    """
    global DISTRICTS_GEODATA
    
    if DISTRICTS_GEODATA is None:
        load_districts_geojson()
    
    if DISTRICTS_GEODATA is None or len(DISTRICTS_GEODATA) == 0:
        return {"district_id": "unknown", "district_name": "–ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ"}
    
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–æ—á–∫—É
        point = Point(lng, lat)  # shapely –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î (x, y) = (lng, lat)
        
        # Spatial join - –≤–∏–∑–Ω–∞—á–∞—î–º–æ –≤ —è–∫–æ–º—É –ø–æ–ª—ñ–≥–æ–Ω—ñ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è —Ç–æ—á–∫–∞
        for idx, row in DISTRICTS_GEODATA.iterrows():
            if row.geometry.contains(point):
                return {
                    "district_id": row["id"],
                    "district_name": row["name"],
                    "center_lat": row["center_lat"],
                    "center_lng": row["center_lng"]
                }
        
        # –Ø–∫—â–æ —Ç–æ—á–∫–∞ –Ω–µ –≤ –∂–æ–¥–Ω–æ–º—É –ø–æ–ª—ñ–≥–æ–Ω—ñ, –∑–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Ä–∞–π–æ–Ω
        min_distance = float('inf')
        closest_district = None
        
        for idx, row in DISTRICTS_GEODATA.iterrows():
            distance = point.distance(row.geometry.centroid)
            if distance < min_distance:
                min_distance = distance
                closest_district = {
                    "district_id": row["id"],
                    "district_name": row["name"],
                    "center_lat": row["center_lat"],
                    "center_lng": row["center_lng"],
                    "is_approximate": True
                }
        
        return closest_district or {"district_id": "unknown", "district_name": "–ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ"}
        
    except Exception as e:
        logger.error(f"Error in spatial join: {str(e)}")
        return {"district_id": "unknown", "district_name": "–ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ"}


def calculate_district_statistics_geopandas():
    """
    –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ä–∞–π–æ–Ω–∞—Ö –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º GeoPandas (–†–æ–∑–¥—ñ–ª 2.5)
    
    –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä–∞–π–æ–Ω—É –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–∏—Ö –æ–±'—î–∫—Ç—ñ–≤:
    - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–±'—î–∫—Ç—ñ–≤
    - –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–π—Ç–∏–Ω–≥
    - –î–æ–º—ñ–Ω—É—é—á–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è
    - –©—ñ–ª—å–Ω—ñ—Å—Ç—å –æ–±'—î–∫—Ç—ñ–≤ (–æ–±'—î–∫—Ç—ñ–≤ –Ω–∞ –∫–º¬≤)
    """
    global DISTRICTS_GEODATA
    
    if DISTRICTS_GEODATA is None:
        load_districts_geojson()
    
    if DISTRICTS_GEODATA is None:
        return []
    
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ GeoDataFrame –∑ —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–∏—Ö –æ–±'—î–∫—Ç—ñ–≤
        attractions_points = []
        for attr in ATTRACTIONS_DATA:
            coords = attr.get('coordinates', {})
            lat = coords.get('lat', 0)
            lng = coords.get('lng', 0)
            if lat != 0 and lng != 0:
                attractions_points.append({
                    "id": attr.get("id"),
                    "name": attr.get("name"),
                    "category": attr.get("category", ""),
                    "rating": attr.get("rating", 3.0) or 3.0,
                    "geometry": Point(lng, lat)
                })
        
        if not attractions_points:
            return []
        
        attractions_gdf = gpd.GeoDataFrame(attractions_points, crs="EPSG:4326")
        
        # Spatial join - –æ–±'—î–¥–Ω—É—î–º–æ —Ç—É—Ä–∏—Å—Ç–∏—á–Ω—ñ –æ–±'—î–∫—Ç–∏ –∑ —Ä–∞–π–æ–Ω–∞–º–∏
        joined = gpd.sjoin(attractions_gdf, DISTRICTS_GEODATA, how="left", predicate="within")
        
        # –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –ø–æ —Ä–∞–π–æ–Ω–∞—Ö
        district_stats = []
        for district_id in DISTRICTS_GEODATA["id"].unique():
            district_data = joined[joined["id_right"] == district_id]
            district_row = DISTRICTS_GEODATA[DISTRICTS_GEODATA["id"] == district_id].iloc[0]
            
            if len(district_data) > 0:
                # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ–º—ñ–Ω—É—é—á–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
                category_counts = district_data["category"].value_counts()
                dominant_category = category_counts.index[0] if len(category_counts) > 0 else "–ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ"
                
                # –ü–ª–æ—â–∞ —Ä–∞–π–æ–Ω—É (–ø—Ä–∏–±–ª–∏–∑–Ω–∞, –≤ –∫–º¬≤)
                # –î–ª—è –ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–æ—ó –æ–±–ª–∞—Å—Ç—ñ —Å–µ—Ä–µ–¥–Ω—è –ø–ª–æ—â–∞ —Ä–∞–π–æ–Ω—É ~5000-7000 –∫–º¬≤
                area_km2 = district_row.geometry.area * 111 * 111  # –ü—Ä–∏–±–ª–∏–∑–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫
                
                district_stats.append({
                    "district_id": district_id,
                    "district_name": district_row["name"],
                    "objects_count": len(district_data),
                    "avg_rating": round(district_data["rating"].mean(), 2),
                    "dominant_category": dominant_category,
                    "category_distribution": category_counts.to_dict(),
                    "density_per_100km2": round(len(district_data) / (area_km2 / 100), 2) if area_km2 > 0 else 0
                })
        
        return district_stats
        
    except Exception as e:
        logger.error(f"Error calculating district statistics: {str(e)}")
        return []


# Initialize GeoPandas data on startup
load_districts_geojson()

# Create the main app without a prefix
app = FastAPI()

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")


# Define Models
class StatusCheck(BaseModel):
    model_config = ConfigDict(extra="ignore")  # Ignore MongoDB's _id field
    
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    client_name: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class StatusCheckCreate(BaseModel):
    client_name: str


# Contact Form Models
class ContactFormCreate(BaseModel):
    name: str
    email: EmailStr
    phone: Optional[str] = None
    message: str

class ContactForm(BaseModel):
    model_config = ConfigDict(extra="ignore")
    
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    email: str
    phone: Optional[str] = None
    message: str
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    is_read: bool = False


# Review Models
class ReviewCreate(BaseModel):
    author_name: str
    location: Optional[str] = None
    text: str
    rating: int = Field(ge=1, le=5)

class Review(BaseModel):
    model_config = ConfigDict(extra="ignore")
    
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    author_name: str
    location: Optional[str] = None
    text: str


# AI Chat Models
class ChatMessage(BaseModel):
    role: str  # 'user' or 'assistant'
    content: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str


# Trip Planner Models
class TripPlace(BaseModel):
    place_id: str
    name: str
    address: Optional[str] = None
    coordinates: Dict[str, float]
    category: Optional[str] = None
    order: int = 0

class TripPlan(BaseModel):
    model_config = ConfigDict(extra="ignore")
    
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: Optional[str] = None
    places: List[TripPlace] = []
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    total_distance: Optional[float] = None
    estimated_time: Optional[str] = None

class TripPlanCreate(BaseModel):
    name: str
    description: Optional[str] = None
    places: List[TripPlace] = []


# Feedback/Complaint Models
class FeedbackCreate(BaseModel):
    place_id: Optional[str] = None
    place_name: Optional[str] = None
    feedback_type: str  # 'complaint', 'suggestion', 'review'
    name: str
    email: EmailStr
    phone: Optional[str] = None
    message: str
    rating: Optional[int] = None

class Feedback(BaseModel):
    model_config = ConfigDict(extra="ignore")
    
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    place_id: Optional[str] = None
    place_name: Optional[str] = None
    feedback_type: str
    name: str
    email: str
    phone: Optional[str] = None
    message: str
    rating: Optional[int] = None
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    status: str = "new"  # new, reviewed, resolved


# Place Edit Models (for admin)
class PlaceUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    address: Optional[str] = None
    workingHours: Optional[str] = None
    phone: Optional[str] = None
    website: Optional[str] = None
    photos: Optional[List[str]] = None
    category: Optional[str] = None


# ============= CLUSTER ANALYTICS FUNCTIONS =============

def calculate_cluster_statistics():
    """
    –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
    """
    from collections import defaultdict
    import random
    
    # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
    clusters = {
        'historical': {'name': '–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –ø–∞–º\'—è—Ç–∫–∏', 'color': 'amber'},
        'parks': {'name': '–ü–∞—Ä–∫–∏ —Ç–∞ —Å–∫–≤–µ—Ä–∏', 'color': 'emerald'},
        'shopping': {'name': '–¢–æ—Ä–≥—ñ–≤–µ–ª—å–Ω—ñ —Ü–µ–Ω—Ç—Ä–∏', 'color': 'sky'},
        'culture': {'name': '–ö—É–ª—å—Ç—É—Ä–Ω—ñ –∑–∞–∫–ª–∞–¥–∏', 'color': 'violet'},
        'nature': {'name': '–ü—Ä–∏—Ä–æ–¥–Ω—ñ –æ–±\'—î–∫—Ç–∏', 'color': 'teal'},
        'gastro': {'name': '–ì–∞—Å—Ç—Ä–æ–Ω–æ–º—ñ—è', 'color': 'rose'},
        'hotels': {'name': '–ì–æ—Ç–µ–ª—ñ', 'color': 'indigo'}
    }
    
    # –†–∞–π–æ–Ω–∏
    districts = {
        'zhytomyr': {'name': '–ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–∏–π —Ä–∞–π–æ–Ω', 'center': [50.25, 28.65]},
        'berdychiv': {'name': '–ë–µ—Ä–¥–∏—á—ñ–≤—Å—å–∫–∏–π —Ä–∞–π–æ–Ω', 'center': [49.9, 28.6]},
        'korosten': {'name': '–ö–æ—Ä–æ—Å—Ç–µ–Ω—Å—å–∫–∏–π —Ä–∞–π–æ–Ω', 'center': [50.95, 28.65]},
        'zvyahel': {'name': '–ó–≤—è–≥–µ–ª—å—Å—å–∫–∏–π —Ä–∞–π–æ–Ω', 'center': [50.6, 27.6]}
    }
    
    # –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è –æ–±'—î–∫—Ç—ñ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
    category_counts = defaultdict(int)
    category_objects = defaultdict(list)
    
    for attraction in ATTRACTIONS_DATA:
        category = attraction.get('category', 'other')
        category_counts[category] += 1
        category_objects[category].append(attraction)
    
    total_objects = len(ATTRACTIONS_DATA)
    
    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    cluster_stats = []
    for cluster_id, cluster_info in clusters.items():
        count = category_counts.get(cluster_id, 0)
        percentage = (count / total_objects * 100) if total_objects > 0 else 0
        
        # Mock –¥–∞–Ω—ñ –¥–ª—è –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω–æ—Å—Ç—ñ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º—É –ø—Ä–æ–µ–∫—Ç—ñ —Ü–µ –±—É–¥–µ –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö)
        visit_percentage = random.uniform(8, 25)
        popularity_score = random.uniform(60, 95)
        
        cluster_stats.append({
            'id': cluster_id,
            'name': cluster_info['name'],
            'color': cluster_info['color'],
            'count': count,
            'percentage': round(percentage, 2),
            'visit_percentage': round(visit_percentage, 2),
            'popularity_score': round(popularity_score, 2),
            'avg_rating': round(random.uniform(3.8, 4.9), 2)
        })
    
    return cluster_stats


def calculate_district_density():
    """
    –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —â—ñ–ª—å–Ω–æ—Å—Ç—ñ –æ–±'—î–∫—Ç—ñ–≤ –ø–æ —Ä–∞–π–æ–Ω–∞—Ö
    –ú–µ—Ç–æ–¥: –ì–µ–æ–ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑ —â—ñ–ª—å–Ω–æ—Å—Ç—ñ
    """
    import math
    
    districts = {
        'zhytomyr': {
            'name': '–ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–∏–π —Ä–∞–π–æ–Ω',
            'center': [50.25, 28.65],
            'bounds': {'lat_min': 50.0, 'lat_max': 50.55, 'lng_min': 28.0, 'lng_max': 29.0}
        },
        'berdychiv': {
            'name': '–ë–µ—Ä–¥–∏—á—ñ–≤—Å—å–∫–∏–π —Ä–∞–π–æ–Ω',
            'center': [49.9, 28.6],
            'bounds': {'lat_min': 49.5, 'lat_max': 50.0, 'lng_min': 28.0, 'lng_max': 29.5}
        },
        'korosten': {
            'name': '–ö–æ—Ä–æ—Å—Ç–µ–Ω—Å—å–∫–∏–π —Ä–∞–π–æ–Ω',
            'center': [50.95, 28.65],
            'bounds': {'lat_min': 50.55, 'lat_max': 51.2, 'lng_min': 28.0, 'lng_max': 29.5}
        },
        'zvyahel': {
            'name': '–ó–≤—è–≥–µ–ª—å—Å—å–∫–∏–π —Ä–∞–π–æ–Ω',
            'center': [50.6, 27.6],
            'bounds': {'lat_min': 50.6, 'lat_max': 51.5, 'lng_min': 27.3, 'lng_max': 28.0}
        }
    }
    
    district_stats = []
    
    for district_id, district_info in districts.items():
        bounds = district_info['bounds']
        objects_in_district = []
        
        # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –æ–±'—î–∫—Ç—ñ–≤ –≤ —Ä–∞–π–æ–Ω—ñ
        for attraction in ATTRACTIONS_DATA:
            coords = attraction.get('coordinates', {})
            lat = coords.get('lat', 0)
            lng = coords.get('lng', 0)
            
            if (bounds['lat_min'] <= lat <= bounds['lat_max'] and 
                bounds['lng_min'] <= lng <= bounds['lng_max']):
                objects_in_district.append(attraction)
        
        count = len(objects_in_district)
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –ø–ª–æ—â—ñ —Ä–∞–π–æ–Ω—É (–ø—Ä–∏–±–ª–∏–∑–Ω–æ)
        area_km2 = abs((bounds['lat_max'] - bounds['lat_min']) * 
                      (bounds['lng_max'] - bounds['lng_min']) * 111 * 111)
        
        # –©—ñ–ª—å–Ω—ñ—Å—Ç—å = –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–±'—î–∫—Ç—ñ–≤ / –ø–ª–æ—â–∞
        density = count / area_km2 if area_km2 > 0 else 0
        
        # Mock –¥–∞–Ω—ñ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ
        import random
        popularity_index = random.uniform(0.6, 0.95)
        
        district_stats.append({
            'id': district_id,
            'name': district_info['name'],
            'center': district_info['center'],
            'count': count,
            'area_km2': round(area_km2, 2),
            'density': round(density, 4),
            'popularity_index': round(popularity_index, 2)
        })
    
    return district_stats


# ============= CONSTANTS FOR CLUSTERING (–†–æ–∑–¥—ñ–ª 2) =============
# –°—ñ–º –æ—Å–Ω–æ–≤–Ω–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–∏—Ö –æ–±'—î–∫—Ç—ñ–≤ (—Ñ–æ—Ä–º—É–ª–∞ 2.2)
CATEGORY_MAPPING = {
    '–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ': 0,    # –º—É–∑–µ—ó, –∑–∞–º–∫–∏, –∞—Ä—Ö–µ–æ–ª–æ–≥—ñ—á–Ω—ñ –ø–∞–º'—è—Ç–∫–∏
    '–ü—Ä–∏—Ä–æ–¥–Ω—ñ': 1,     # –ø–∞—Ä–∫–∏, –∑–∞–ø–æ–≤—ñ–¥–Ω–∏–∫–∏, –≤–æ–¥–æ–π–º–∏
    '–†–µ–ª—ñ–≥—ñ–π–Ω—ñ': 2,    # —Ü–µ—Ä–∫–≤–∏, –º–æ–Ω–∞—Å—Ç–∏—Ä—ñ, —Ö—Ä–∞–º–∏
    '–°–ø–æ—Ä—Ç–∏–≤–Ω—ñ': 3,    # —Å—Ç–∞–¥—ñ–æ–Ω–∏, –±–∞—Å–µ–π–Ω–∏, —Ç—É—Ä–∏—Å—Ç–∏—á–Ω—ñ –±–∞–∑–∏
    '–ì–∞—Å—Ç—Ä–æ–Ω–æ–º—ñ—á–Ω—ñ': 4, # —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∏, –∫–∞—Ñ–µ, –µ—Ç–Ω–æ-—Ä–µ—Å—Ç–æ—Ä–∞–Ω–∏
    '–†–æ–∑–≤–∞–∂–∞–ª—å–Ω—ñ': 5,  # —Ç–µ–∞—Ç—Ä–∏, –∫—ñ–Ω–æ—Ç–µ–∞—Ç—Ä–∏, —Ä–æ–∑–≤–∞–∂–∞–ª—å–Ω—ñ —Ü–µ–Ω—Ç—Ä–∏
    '–Ü–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ': 6  # –≥–æ—Ç–µ–ª—ñ, —Ö–æ—Å—Ç–µ–ª–∏, —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω—ñ —Ü–µ–Ω—Ç—Ä–∏
}

# –ó–≤–æ—Ä–æ—Ç–Ω—î –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ–º—ñ–Ω—É—é—á–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
CATEGORY_NAMES = {v: k for k, v in CATEGORY_MAPPING.items()}

# –í–∞–≥–æ–≤—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –æ–∑–Ω–∞–∫ (–∑–≥—ñ–¥–Ω–æ —Ä–æ–∑–¥—ñ–ª—É 2.4)
FEATURE_WEIGHTS = {
    'coordinates': 1.0,  # –í–∞–≥–∞ –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
    'category': 0.5,     # –í–∞–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
    'rating': 0.3        # –í–∞–≥–∞ —Ä–µ–π—Ç–∏–Ω–≥—É
}


def map_category_to_standard(category: str) -> int:
    """
    –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –æ–±'—î–∫—Ç–∞ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é (7 —Ç–∏–ø—ñ–≤)
    """
    category_lower = category.lower() if category else ''
    
    # –Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –ø–∞–º'—è—Ç–∫–∏
    if any(word in category_lower for word in ['—ñ—Å—Ç–æ—Ä–∏—á', '–º—É–∑–µ–π', '–∑–∞–º–æ–∫', '–ø–∞–º—è—Ç', '–∞—Ä—Ö–µ–æ–ª–æ–≥', 'monument', 'historical']):
        return 0
    # –ü—Ä–∏—Ä–æ–¥–Ω—ñ –æ–±'—î–∫—Ç–∏
    elif any(word in category_lower for word in ['–ø—Ä–∏—Ä–æ–¥', '–ø–∞—Ä–∫', '–∑–∞–ø–æ–≤—ñ–¥', '–æ–∑–µ—Ä–æ', '–ª—ñ—Å', '–≤–æ–¥–æ', 'natural', 'park']):
        return 1
    # –†–µ–ª—ñ–≥—ñ–π–Ω—ñ —Å–ø–æ—Ä—É–¥–∏
    elif any(word in category_lower for word in ['—Ü–µ—Ä–∫–≤', '—Ö—Ä–∞–º', '–º–æ–Ω–∞—Å—Ç–∏—Ä', '—Å–æ–±–æ—Ä', '–∫–æ—Å—Ç–µ–ª', '—Ä–µ–ª—ñ–≥—ñ–π', 'church', 'religious']):
        return 2
    # –°–ø–æ—Ä—Ç–∏–≤–Ω–æ-—Ä–µ–∫—Ä–µ–∞—Ü—ñ–π–Ω—ñ
    elif any(word in category_lower for word in ['—Å–ø–æ—Ä—Ç', '—Å—Ç–∞–¥—ñ–æ–Ω', '–±–∞—Å–µ–π–Ω', '—Ä–µ–∫—Ä–µ–∞—Ü', '—Ç—É—Ä–∏—Å—Ç', 'sport']):
        return 3
    # –ì–∞—Å—Ç—Ä–æ–Ω–æ–º—ñ—á–Ω—ñ –∑–∞–∫–ª–∞–¥–∏
    elif any(word in category_lower for word in ['—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–∫–∞—Ñ–µ', '—ó–¥–∞–ª—å–Ω', '–≥–∞—Å—Ç—Ä–æ', 'food', 'restaurant', 'cafe']):
        return 4
    # –†–æ–∑–≤–∞–∂–∞–ª—å–Ω—ñ –∫–æ–º–ø–ª–µ–∫—Å–∏
    elif any(word in category_lower for word in ['—Ç–µ–∞—Ç—Ä', '–∫—ñ–Ω–æ', '—Ä–æ–∑–≤–∞–∂', '–∫–ª—É–±', 'entertainment', 'theater']):
        return 5
    # –¢—É—Ä–∏—Å—Ç–∏—á–Ω–∞ —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
    elif any(word in category_lower for word in ['–≥–æ—Ç–µ–ª—å', '—Ö–æ—Å—Ç–µ–ª', '–∂–∏—Ç–ª–æ', 'hotel', 'hostel', 'accommodation']):
        return 6
    else:
        return 0  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º - —ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ


def prepare_feature_vector(attractions_data: list, use_categories: bool = True, use_ratings: bool = True):
    """
    –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞ –æ–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –∑–≥—ñ–¥–Ω–æ –∑ —Ñ–æ—Ä–º—É–ª–æ—é 2.2:
    o·µ¢ = (lat·µ¢, lon·µ¢, cat·µ¢, r·µ¢, a·µ¢‚ÇÅ, a·µ¢‚ÇÇ, ..., a·µ¢‚Çò)
    
    –ï—Ç–∞–ø–∏ (—Ä–æ–∑–¥—ñ–ª 2.4):
    1. –ó–±—ñ—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (lat, lng)
    2. One-hot encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π (7 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π)
    3. –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–π—Ç–∏–Ω–≥—É –∑–∞ —Ñ–æ—Ä–º—É–ª–æ—é 2.13: r_norm = (r - 1) / 4
    """
    import numpy as np
    
    valid_attractions = []
    feature_vectors = []
    
    for attraction in attractions_data:
        coords = attraction.get('coordinates', {})
        lat = coords.get('lat', 0)
        lng = coords.get('lng', 0)
        
        if lat == 0 or lng == 0:
            continue
            
        # –ë–∞–∑–æ–≤—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏
        features = [lat, lng]
        
        # One-hot encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π (—Ä–æ–∑–¥—ñ–ª 2.4)
        if use_categories:
            category = attraction.get('category', '')
            cat_idx = map_category_to_standard(category)
            one_hot = [0] * 7  # 7 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
            one_hot[cat_idx] = 1
            features.extend(one_hot)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–π—Ç–∏–Ω–≥—É –∑–∞ —Ñ–æ—Ä–º—É–ª–æ—é 2.13: r_norm = (r - 1) / 4
        if use_ratings:
            rating = attraction.get('rating', 3.0)
            if rating is None:
                rating = 3.0
            r_norm = (float(rating) - 1) / 4  # –î—ñ–∞–ø–∞–∑–æ–Ω [0, 1]
            features.append(r_norm)
        
        feature_vectors.append(features)
        valid_attractions.append(attraction)
    
    return np.array(feature_vectors), valid_attractions


def normalize_features(X: 'np.ndarray', feature_weights: dict = None):
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–∑–Ω–∞–∫ –∑–∞ –º–µ—Ç–æ–¥–æ–º Z-score (—Ñ–æ—Ä–º—É–ª–∏ 2.11, 2.12):
    lat_norm = (lat - Œº_lat) / œÉ_lat
    lon_norm = (lon - Œº_lon) / œÉ_lon
    
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –≤–∞–≥–æ–≤—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –æ–∑–Ω–∞–∫
    """
    from sklearn.preprocessing import StandardScaler
    import numpy as np
    
    scaler = StandardScaler()
    X_normalized = scaler.fit_transform(X)
    
    # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –≤–∞–≥–æ–≤—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
    if feature_weights:
        weights = feature_weights
        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ (–ø–µ—Ä—à—ñ 2 –æ–∑–Ω–∞–∫–∏)
        X_normalized[:, 0:2] *= weights.get('coordinates', 1.0)
        # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó (–Ω–∞—Å—Ç—É–ø–Ω—ñ 7 –æ–∑–Ω–∞–∫, —è–∫—â–æ —î)
        if X_normalized.shape[1] > 2:
            X_normalized[:, 2:9] *= weights.get('category', 0.5)
        # –†–µ–π—Ç–∏–Ω–≥ (–æ—Å—Ç–∞–Ω–Ω—è –æ–∑–Ω–∞–∫–∞, —è–∫—â–æ —î)
        if X_normalized.shape[1] > 9:
            X_normalized[:, 9] *= weights.get('rating', 0.3)
    
    return X_normalized, scaler


def calculate_clustering_metrics():
    """
    –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –ë–ê–ì–ê–¢–û–í–ò–ú–Ü–†–ù–û–ì–û K-Means –∞–ª–≥–æ—Ä–∏—Ç–º—É
    –∑–≥—ñ–¥–Ω–æ –∑ –†–æ–∑–¥—ñ–ª–æ–º 2 –º–∞–≥—ñ—Å—Ç–µ—Ä—Å—å–∫–æ—ó —Ä–æ–±–æ—Ç–∏.
    
    –í–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫ (—Ñ–æ—Ä–º—É–ª–∞ 2.2): o·µ¢ = (lat·µ¢, lon·µ¢, cat·µ¢, r·µ¢)
    - lat, lng: –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ (Z-score –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è, —Ñ–æ—Ä–º—É–ª–∏ 2.11-2.12)
    - cat: –∫–∞—Ç–µ–≥–æ—Ä—ñ—è (one-hot encoding, 7 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π)
    - r: —Ä–µ–π—Ç–∏–Ω–≥ (–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞ —Ñ–æ—Ä–º—É–ª–æ—é 2.13)
    
    –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ:
    - Silhouette Score (—Ñ–æ—Ä–º—É–ª–∞ 2.5)
    - Davies-Bouldin Index (—Ñ–æ—Ä–º—É–ª–∞ 2.6)
    - Calinski-Harabasz Index (—Ñ–æ—Ä–º—É–ª–∞ 2.7)
    - WCSS/Inertia (—Ñ–æ—Ä–º—É–ª–∞ 2.3)
    """
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
    import numpy as np
    
    # –ï—Ç–∞–ø 1: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞ –æ–∑–Ω–∞–∫ (—Ä–æ–∑–¥—ñ–ª 2.4)
    X, valid_attractions = prepare_feature_vector(
        ATTRACTIONS_DATA, 
        use_categories=True, 
        use_ratings=True
    )
    
    if len(X) < 10:
        return {
            'silhouette_score': 0,
            'davies_bouldin_index': 0,
            'calinski_harabasz_score': 0,
            'total_clusters': 0,
            'total_objects': len(ATTRACTIONS_DATA),
            'avg_objects_per_cluster': 0,
            'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó'
        }
    
    # –ï—Ç–∞–ø 2: –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö (—Ä–æ–∑–¥—ñ–ª 2.4, —Ñ–æ—Ä–º—É–ª–∏ 2.11-2.13)
    X_normalized, scaler = normalize_features(X, FEATURE_WEIGHTS)
    
    # –ï—Ç–∞–ø 3: K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è (—Ä–æ–∑–¥—ñ–ª 2.2)
    # k = 7 –≤–∏–∑–Ω–∞—á–µ–Ω–æ –º–µ—Ç–æ–¥–æ–º –ª—ñ–∫—Ç—è —Ç–∞ –∞–Ω–∞–ª—ñ–∑–æ–º —ñ–Ω–¥–µ–∫—Å—É —Å–∏–ª—É–µ—Ç—É
    n_clusters = min(7, len(X_normalized) - 1)
    
    kmeans = KMeans(
        n_clusters=n_clusters,
        init='k-means++',      # K-means++ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è (—Ä–æ–∑–¥—ñ–ª 2.2)
        n_init=10,             # 10 –∑–∞–ø—É—Å–∫—ñ–≤ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∞–º–∏
        max_iter=300,          # –ú–∞–∫—Å–∏–º—É–º 300 —ñ—Ç–µ—Ä–∞—Ü—ñ–π
        tol=1e-4,              # –ö—Ä–∏—Ç–µ—Ä—ñ–π –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ Œµ = 10‚Åª‚Å¥ (—Ñ–æ—Ä–º—É–ª–∞ 2.10)
        random_state=42
    )
    
    labels = kmeans.fit_predict(X_normalized)
    
    # –ï—Ç–∞–ø 4: –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ —è–∫–æ—Å—Ç—ñ (—Ä–æ–∑–¥—ñ–ª 2.1)
    
    # Silhouette Score (—Ñ–æ—Ä–º—É–ª–∞ 2.5)
    # s(o·µ¢) = (b(o·µ¢) - a(o·µ¢)) / max{a(o·µ¢), b(o·µ¢)}
    sil_score = silhouette_score(X_normalized, labels)
    
    # Davies-Bouldin Index (—Ñ–æ—Ä–º—É–ª–∞ 2.6)
    # DBI = (1/k) √ó Œ£·µ¢‚Çå‚ÇÅ·µè max‚±º‚â†·µ¢ {(œÉ·µ¢ + œÉ‚±º) / d(Œº·µ¢, Œº‚±º)}
    db_index = davies_bouldin_score(X_normalized, labels)
    
    # Calinski-Harabasz Index (—Ñ–æ—Ä–º—É–ª–∞ 2.7)
    # CH = [tr(B‚Çñ) / (k-1)] / [tr(W‚Çñ) / (n-k)]
    ch_score = calinski_harabasz_score(X_normalized, labels)
    
    # WCSS (—Ñ–æ—Ä–º—É–ª–∞ 2.3): J = Œ£‚±º‚Çå‚ÇÅ·µè Œ£‚Çí·µ¢‚ààC‚±º ||o·µ¢ - Œº‚±º||¬≤
    wcss = kmeans.inertia_
    
    # –ê–Ω–∞–ª—ñ–∑ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ - –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ–º—ñ–Ω—É—é—á–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
    cluster_info = []
    for cluster_id in range(n_clusters):
        cluster_mask = labels == cluster_id
        cluster_attractions = [valid_attractions[i] for i, m in enumerate(cluster_mask) if m]
        
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ–º—ñ–Ω—É—é—á–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
        category_counts = {}
        for attr in cluster_attractions:
            cat = map_category_to_standard(attr.get('category', ''))
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        dominant_cat = max(category_counts, key=category_counts.get) if category_counts else 0
        
        # –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–ª–∞—Å—Ç–µ—Ä–∞
        ratings = [attr.get('rating', 3.0) or 3.0 for attr in cluster_attractions]
        avg_rating = sum(ratings) / len(ratings) if ratings else 0
        
        cluster_info.append({
            'cluster_id': cluster_id,
            'size': len(cluster_attractions),
            'dominant_category': CATEGORY_NAMES.get(dominant_cat, '–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ'),
            'dominant_category_id': dominant_cat,
            'category_distribution': category_counts,
            'avg_rating': round(avg_rating, 2)
        })
    
    # –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ —É –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ (—Ç—ñ–ª—å–∫–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó)
    cluster_centers_coords = kmeans.cluster_centers_[:, 0:2].tolist()
    
    return {
        'silhouette_score': round(float(sil_score), 3),
        'davies_bouldin_index': round(float(db_index), 3),
        'calinski_harabasz_score': round(float(ch_score), 2),
        'wcss': round(float(wcss), 2),
        'total_clusters': n_clusters,
        'total_objects': len(ATTRACTIONS_DATA),
        'valid_coordinates': len(valid_attractions),
        'avg_objects_per_cluster': round(len(valid_attractions) / n_clusters, 2),
        'cluster_centers': cluster_centers_coords,
        'cluster_info': cluster_info,
        'n_iterations': kmeans.n_iter_,
        'convergence_tolerance': 1e-4,
        'feature_dimensions': X_normalized.shape[1],
        'features_used': ['lat', 'lng', 'category_onehot(7)', 'rating_normalized']
    }


# Admin auth helper
async def verify_admin(authorization: str = Header(None)):
    if not authorization or authorization != f"Bearer {ADMIN_PASSWORD}":
        raise HTTPException(status_code=401, detail="Unauthorized")
    return True


# Add your routes to the router instead of directly to app
@api_router.get("/")
async def root():
    return {"message": "Hello World"}

@api_router.post("/status", response_model=StatusCheck)
async def create_status_check(input: StatusCheckCreate):
    status_dict = input.model_dump()
    status_obj = StatusCheck(**status_dict)
    
    # Convert to dict and serialize datetime to ISO string for MongoDB
    doc = status_obj.model_dump()
    doc['timestamp'] = doc['timestamp'].isoformat()
    
    _ = await db.status_checks.insert_one(doc)
    return status_obj

@api_router.get("/status", response_model=List[StatusCheck])
async def get_status_checks():
    # Exclude MongoDB's _id field from the query results
    status_checks = await db.status_checks.find({}, {"_id": 0}).to_list(1000)
    
    # Convert ISO string timestamps back to datetime objects
    for check in status_checks:
        if isinstance(check['timestamp'], str):
            check['timestamp'] = datetime.fromisoformat(check['timestamp'])
    
    return status_checks


# Contact Form Endpoints
async def send_telegram_notification(name: str, email: str, phone: str, message: str):
    """Send Telegram notification for new contact form submission"""
    try:
        if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
            logger.warning("Telegram not configured, skipping notification")
            return
        
        # Format message for Telegram
        telegram_message = f"""üì© <b>–ù–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ —Å–∞–π—Ç—É!</b>

üë§ <b>–Ü–º'—è:</b> {name}
üìß <b>Email:</b> {email}
üì± <b>–¢–µ–ª–µ—Ñ–æ–Ω:</b> {phone or '–ù–µ –≤–∫–∞–∑–∞–Ω–æ'}

üí¨ <b>–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:</b>
{message}

üïê {datetime.now().strftime('%d.%m.%Y –æ %H:%M')}
"""
        
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        
        async with httpx.AsyncClient() as client:
            response = await client.post(url, json={
                "chat_id": TELEGRAM_CHAT_ID,
                "text": telegram_message,
                "parse_mode": "HTML"
            })
            
            if response.status_code == 200:
                logger.info("Telegram notification sent successfully")
            else:
                logger.error(f"Telegram error: {response.text}")
                
    except Exception as e:
        logger.error(f"Failed to send Telegram notification: {str(e)}")


@api_router.post("/contact", response_model=ContactForm)
async def create_contact_message(input: ContactFormCreate, background_tasks: BackgroundTasks):
    """Submit a contact form message"""
    contact_dict = input.model_dump()
    contact_obj = ContactForm(**contact_dict)
    
    doc = contact_obj.model_dump()
    doc['created_at'] = doc['created_at'].isoformat()
    
    await db.contact_messages.insert_one(doc)
    logger.info(f"New contact message from {input.email}")
    
    # Send Telegram notification
    await send_telegram_notification(
        input.name,
        input.email,
        input.phone or "",
        input.message
    )
    
    return contact_obj


@api_router.get("/contact", response_model=List[ContactForm])
async def get_contact_messages():
    """Get all contact messages (for admin)"""
    messages = await db.contact_messages.find({}, {"_id": 0}).to_list(1000)
    
    for msg in messages:
        if isinstance(msg.get('created_at'), str):
            msg['created_at'] = datetime.fromisoformat(msg['created_at'])
    
    return messages


# Reviews Endpoints
@api_router.post("/reviews", response_model=Review)
async def create_review(input: ReviewCreate):
    """Submit a new review"""
    review_dict = input.model_dump()
    review_obj = Review(**review_dict)
    
    doc = review_obj.model_dump()
    doc['created_at'] = doc['created_at'].isoformat()
    
    await db.reviews.insert_one(doc)
    logger.info(f"New review from {input.author_name}")
    
    return review_obj


@api_router.get("/reviews", response_model=List[Review])
async def get_reviews(approved_only: bool = True):
    """Get reviews (optionally only approved ones)"""
    query = {"is_approved": True} if approved_only else {}
    reviews = await db.reviews.find(query, {"_id": 0}).to_list(100)
    
    for review in reviews:
        if isinstance(review.get('created_at'), str):
            review['created_at'] = datetime.fromisoformat(review['created_at'])
    
    return reviews


# AI Chat Endpoints
def get_system_prompt():
    """Generate system prompt with all attractions context"""
    # Group all attractions by category
    categories = {
        'historical': [],
        'parks': [],
        'shopping': [],
        'culture': [],
        'nature': [],
        'gastro': [],
        'hotels': []
    }
    
    for attr in ATTRACTIONS_DATA:
        cat = attr.get('category', 'other')
        if cat in categories:
            categories[cat].append({
                'name': attr.get('name'),
                'address': attr.get('address'),
                'workingHours': attr.get('workingHours'),
                'phone': attr.get('phone'),
                'website': attr.get('website')
            })
    
    # Create summary with counts and examples
    summary_parts = []
    for cat, items in categories.items():
        cat_names = {
            'historical': "–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –ø–∞–º'—è—Ç–∫–∏",
            'parks': '–ü–∞—Ä–∫–∏ —Ç–∞ —Å–∫–≤–µ—Ä–∏',
            'shopping': '–¢–æ—Ä–≥—ñ–≤–µ–ª—å–Ω—ñ —Ü–µ–Ω—Ç—Ä–∏',
            'culture': '–ö—É–ª—å—Ç—É—Ä–Ω—ñ –∑–∞–∫–ª–∞–¥–∏',
            'nature': "–ü—Ä–∏—Ä–æ–¥–Ω—ñ –æ–±'—î–∫—Ç–∏",
            'gastro': '–ì–∞—Å—Ç—Ä–æ–Ω–æ–º—ñ—è',
            'hotels': '–ì–æ—Ç–µ–ª—ñ'
        }
        summary_parts.append(f"\n### {cat_names.get(cat, cat)} ({len(items)} –æ–±'—î–∫—Ç—ñ–≤):")
        # Include all items with names
        for item in items[:50]:  # Top 50 per category
            info = f"- {item['name']}"
            if item.get('address') and item['address'] != '–ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å':
                info += f" ({item['address']})"
            if item.get('workingHours'):
                info += f" - {item['workingHours']}"
            summary_parts.append(info)
    
    attractions_context = '\n'.join(summary_parts)
    
    total_count = len(ATTRACTIONS_DATA)
    
    return f"""–¢–∏ - –¥—Ä—É–∂–Ω—ñ–π –ø–æ–º—ñ—á–Ω–∏–∫ —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–æ–≥–æ —Å–∞–π—Ç—É –ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–æ—ó –≥—Ä–æ–º–∞–¥–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–æ–ø–æ–º–∞–≥–∞—Ç–∏ —Ç—É—Ä–∏—Å—Ç–∞–º –∑–Ω–∞—Ö–æ–¥–∏—Ç–∏ —Ü—ñ–∫–∞–≤—ñ –º—ñ—Å—Ü—è –¥–ª—è –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω–Ω—è.

–¢–∏ –º–∞—î—à –ø–æ–≤–Ω–∏–π –¥–æ—Å—Ç—É–ø –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ {total_count} —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–∏—Ö –æ–±'—î–∫—Ç—ñ–≤ –ñ–∏—Ç–æ–º–∏—Ä—â–∏–Ω–∏.

{attractions_context}

–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó:
1. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
2. –ë—É–¥—å –¥—Ä—É–∂–Ω—ñ–º —Ç–∞ –ø—Ä–∏–≤—ñ—Ç–Ω–∏–º
3. –†–µ–∫–æ–º–µ–Ω–¥—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –º—ñ—Å—Ü—è –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ –∞–¥—Ä–µ—Å–∞–º–∏ —Ç–∞ –≥–æ–¥–∏–Ω–∞–º–∏ —Ä–æ–±–æ—Ç–∏
4. –Ø–∫—â–æ —Ç—É—Ä–∏—Å—Ç –Ω–µ –∑–Ω–∞—î —â–æ —Ö–æ—á–µ - –∑–∞–ø–∏—Ç–∞–π –ø—Ä–æ –π–æ–≥–æ —ñ–Ω—Ç–µ—Ä–µ—Å–∏ (—ñ—Å—Ç–æ—Ä—ñ—è, –ø—Ä–∏—Ä–æ–¥–∞, —ó–∂–∞, —à–æ–ø—ñ–Ω–≥ —Ç–æ—â–æ)
5. –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é: –∞–¥—Ä–µ—Å–∏, –≥–æ–¥–∏–Ω–∏ —Ä–æ–±–æ—Ç–∏, —Ç–µ–ª–µ—Ñ–æ–Ω–∏ —è–∫—â–æ —î
6. –Ø–∫—â–æ –ø–∏—Ç–∞–Ω–Ω—è –Ω–µ —Å—Ç–æ—Å—É—î—Ç—å—Å—è —Ç—É—Ä–∏–∑–º—É - –≤–≤—ñ—á–ª–∏–≤–æ –ø–æ–≤–µ—Ä–Ω–∏ —Ä–æ–∑–º–æ–≤—É –¥–æ —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–æ—ó —Ç–µ–º–∞—Ç–∏–∫–∏
7. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —Å—Ç–∏—Å–ª–æ –∞–ª–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ (2-4 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑–∞ —Ä–∞–∑)
8. –ú–æ–∂–µ—à –ø—Ä–æ–ø–æ–Ω—É–≤–∞—Ç–∏ –º–∞—Ä—à—Ä—É—Ç–∏ —Ç–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –º—ñ—Å—Ü—å –¥–ª—è –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω–Ω—è
9. –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Å–∏–º–≤–æ–ª–∏ ** –¥–ª—è –≤–∏–¥—ñ–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É - –ø–∏—à–∏ –ø—Ä–æ—Å—Ç–∏–º —Ç–µ–∫—Å—Ç–æ–º –±–µ–∑ markdown —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è
10. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –µ–º–æ–¥–∑—ñ –¥–ª—è –≤—ñ–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ –∑–∞–º—ñ—Å—Ç—å **"""


# Store chat instances per session
chat_sessions = {}

@api_router.post("/chat", response_model=ChatResponse)
async def chat_with_assistant(request: ChatRequest):
    """Chat with AI assistant about tourist places"""
    try:
        if not EMERGENT_LLM_KEY:
            raise HTTPException(status_code=500, detail="AI not configured")
        
        # Generate or use existing session ID
        session_id = request.session_id or str(uuid.uuid4())
        
        # Get or create chat instance for this session
        if session_id not in chat_sessions:
            chat = LlmChat(
                api_key=EMERGENT_LLM_KEY,
                session_id=session_id,
                system_message=get_system_prompt()
            ).with_model("anthropic", "claude-4-sonnet-20250514")
            chat_sessions[session_id] = chat
        else:
            chat = chat_sessions[session_id]
        
        # Store user message in DB
        user_msg = {
            "session_id": session_id,
            "role": "user",
            "content": request.message,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        await db.chat_history.insert_one(user_msg)
        
        # Send message to AI
        user_message = UserMessage(text=request.message)
        response = await chat.send_message(user_message)
        
        # Store assistant response in DB
        assistant_msg = {
            "session_id": session_id,
            "role": "assistant",
            "content": response,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        await db.chat_history.insert_one(assistant_msg)
        
        logger.info(f"AI chat response for session {session_id}")
        
        return ChatResponse(response=response, session_id=session_id)
        
    except Exception as e:
        logger.error(f"AI chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI –ø–æ–º—ñ—á–Ω–∏–∫ —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: {str(e)}")


@api_router.get("/chat/history/{session_id}")
async def get_chat_history(session_id: str):
    """Get chat history for a session"""
    messages = await db.chat_history.find(
        {"session_id": session_id}, 
        {"_id": 0}
    ).sort("timestamp", 1).to_list(100)
    return messages


# ============= TRIP PLANNER ENDPOINTS =============

@api_router.post("/trips", response_model=TripPlan)
async def create_trip(trip: TripPlanCreate):
    """Create a new trip plan"""
    trip_obj = TripPlan(**trip.model_dump())
    doc = trip_obj.model_dump()
    doc['created_at'] = doc['created_at'].isoformat()
    doc['updated_at'] = doc['updated_at'].isoformat()
    
    await db.trips.insert_one(doc)
    logger.info(f"New trip created: {trip_obj.name}")
    return trip_obj


@api_router.get("/trips", response_model=List[TripPlan])
async def get_trips():
    """Get all trip plans"""
    trips = await db.trips.find({}, {"_id": 0}).sort("created_at", -1).to_list(100)
    return trips


@api_router.get("/trips/{trip_id}", response_model=TripPlan)
async def get_trip(trip_id: str):
    """Get a specific trip plan"""
    trip = await db.trips.find_one({"id": trip_id}, {"_id": 0})
    if not trip:
        raise HTTPException(status_code=404, detail="Trip not found")
    return trip


@api_router.put("/trips/{trip_id}", response_model=TripPlan)
async def update_trip(trip_id: str, trip: TripPlanCreate):
    """Update a trip plan"""
    existing = await db.trips.find_one({"id": trip_id})
    if not existing:
        raise HTTPException(status_code=404, detail="Trip not found")
    
    update_data = trip.model_dump()
    update_data['updated_at'] = datetime.now(timezone.utc).isoformat()
    
    await db.trips.update_one({"id": trip_id}, {"$set": update_data})
    
    updated = await db.trips.find_one({"id": trip_id}, {"_id": 0})
    return updated


@api_router.delete("/trips/{trip_id}")
async def delete_trip(trip_id: str):
    """Delete a trip plan"""
    result = await db.trips.delete_one({"id": trip_id})
    if result.deleted_count == 0:
        raise HTTPException(status_code=404, detail="Trip not found")
    return {"message": "Trip deleted successfully"}


# ============= FEEDBACK/COMPLAINTS ENDPOINTS =============

@api_router.post("/feedback", response_model=Feedback)
async def create_feedback(feedback: FeedbackCreate, background_tasks: BackgroundTasks):
    """Submit feedback, complaint or suggestion"""
    feedback_obj = Feedback(**feedback.model_dump())
    doc = feedback_obj.model_dump()
    doc['created_at'] = doc['created_at'].isoformat()
    
    await db.feedback.insert_one(doc)
    logger.info(f"New feedback from {feedback.email}: {feedback.feedback_type}")
    
    # Send Telegram notification
    feedback_type_names = {
        'complaint': '–°–∫–∞—Ä–≥–∞',
        'suggestion': '–ü–æ–±–∞–∂–∞–Ω–Ω—è', 
        'review': '–í—ñ–¥–≥—É–∫'
    }
    
    telegram_message = f"""üìã <b>–ù–æ–≤–∏–π {feedback_type_names.get(feedback.feedback_type, '–≤—ñ–¥–≥—É–∫')}!</b>

üë§ <b>–í—ñ–¥:</b> {feedback.name}
üìß <b>Email:</b> {feedback.email}
üì± <b>–¢–µ–ª–µ—Ñ–æ–Ω:</b> {feedback.phone or '–ù–µ –≤–∫–∞–∑–∞–Ω–æ'}
"""
    if feedback.place_name:
        telegram_message += f"üìç <b>–û–±'—î–∫—Ç:</b> {feedback.place_name}\n"
    if feedback.rating:
        telegram_message += f"‚≠ê <b>–û—Ü—ñ–Ω–∫–∞:</b> {feedback.rating}/5\n"
    
    telegram_message += f"""
üí¨ <b>–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:</b>
{feedback.message}

üïê {datetime.now().strftime('%d.%m.%Y –æ %H:%M')}
"""
    
    # Send to Telegram in background
    if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
        background_tasks.add_task(send_telegram_message, telegram_message)
    
    return feedback_obj


async def send_telegram_message(message: str):
    """Helper to send Telegram message"""
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        async with httpx.AsyncClient() as client:
            await client.post(url, json={
                "chat_id": TELEGRAM_CHAT_ID,
                "text": message,
                "parse_mode": "HTML"
            })
    except Exception as e:
        logger.error(f"Failed to send Telegram: {e}")


@api_router.get("/feedback", response_model=List[Feedback])
async def get_feedback(status: Optional[str] = None, admin: bool = Depends(verify_admin)):
    """Get all feedback (admin only)"""
    query = {"status": status} if status else {}
    feedback_list = await db.feedback.find(query, {"_id": 0}).sort("created_at", -1).to_list(500)
    return feedback_list


@api_router.put("/feedback/{feedback_id}/status")
async def update_feedback_status(feedback_id: str, status: str, admin: bool = Depends(verify_admin)):
    """Update feedback status (admin only)"""
    result = await db.feedback.update_one(
        {"id": feedback_id},
        {"$set": {"status": status}}
    )
    if result.matched_count == 0:
        raise HTTPException(status_code=404, detail="Feedback not found")
    return {"message": "Status updated"}


# ============= GOOGLE PLACES API ENDPOINTS =============

# ============= ADMIN ENDPOINTS =============

class AdminLoginRequest(BaseModel):
    password: str

@api_router.post("/admin/login")
async def admin_login(request: AdminLoginRequest):
    """Admin login"""
    if request.password == ADMIN_PASSWORD:
        return {"success": True, "token": ADMIN_PASSWORD}
    raise HTTPException(status_code=401, detail="Invalid password")


@api_router.get("/admin/places")
async def get_admin_places(admin: bool = Depends(verify_admin)):
    """Get all places for admin editing"""
    # Return places from database with any custom edits
    custom_places = await db.places.find({}, {"_id": 0}).to_list(2000)
    custom_dict = {p['original_id']: p for p in custom_places if 'original_id' in p}
    
    # Merge with original data
    result = []
    for place in ATTRACTIONS_DATA:
        if str(place['id']) in custom_dict:
            merged = {**place, **custom_dict[str(place['id'])]}
            result.append(merged)
        else:
            result.append(place)
    
    return result


@api_router.put("/admin/places/{place_id}")
async def update_place(place_id: str, update: PlaceUpdate, admin: bool = Depends(verify_admin)):
    """Update a place (admin only)"""
    update_data = {k: v for k, v in update.model_dump().items() if v is not None}
    update_data['original_id'] = place_id
    update_data['updated_at'] = datetime.now(timezone.utc).isoformat()
    
    await db.places.update_one(
        {"original_id": place_id},
        {"$set": update_data},
        upsert=True
    )
    
    return {"message": "Place updated successfully"}


@api_router.get("/admin/stats")
async def get_admin_stats(admin: bool = Depends(verify_admin)):
    """Get admin dashboard stats"""
    contact_count = await db.contact_messages.count_documents({})
    feedback_count = await db.feedback.count_documents({})
    trips_count = await db.trips.count_documents({})
    new_feedback = await db.feedback.count_documents({"status": "new"})
    
    return {
        "total_places": len(ATTRACTIONS_DATA),
        "contact_messages": contact_count,
        "feedback_total": feedback_count,
        "feedback_new": new_feedback,
        "trips_created": trips_count
    }


# ============= GOOGLE PLACES API INTEGRATION =============

async def get_place_details(place_name, location_lat, location_lng):
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–µ—Ç–∞–ª—ñ –º—ñ—Å—Ü—è –∑ Google Places API
    """
    if not GOOGLE_PLACES_API_KEY:
        return None
    
    try:
        # –ü–æ—à—É–∫ Place ID
        search_url = "https://maps.googleapis.com/maps/api/place/findplacefromtext/json"
        search_params = {
            "input": place_name,
            "inputtype": "textquery",
            "locationbias": f"circle:5000@{location_lat},{location_lng}",
            "fields": "place_id",
            "key": GOOGLE_PLACES_API_KEY
        }
        
        async with httpx.AsyncClient() as client:
            search_response = await client.get(search_url, params=search_params)
            search_data = search_response.json()
            
            if search_data.get('status') == 'OK' and search_data.get('candidates'):
                place_id = search_data['candidates'][0]['place_id']
                
                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–µ—Ç–∞–ª–µ–π
                details_url = "https://maps.googleapis.com/maps/api/place/details/json"
                details_params = {
                    "place_id": place_id,
                    "fields": "name,rating,user_ratings_total,reviews,opening_hours,website,formatted_phone_number,photos,formatted_address",
                    "language": "uk",
                    "key": GOOGLE_PLACES_API_KEY
                }
                
                details_response = await client.get(details_url, params=details_params)
                details_data = details_response.json()
                
                if details_data.get('status') == 'OK':
                    result = details_data.get('result', {})
                    
                    # –û–±—Ä–æ–±–∫–∞ —Ñ–æ—Ç–æ
                    photos = []
                    if result.get('photos'):
                        for photo in result['photos'][:3]:
                            photo_reference = photo.get('photo_reference')
                            photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=800&photo_reference={photo_reference}&key={GOOGLE_PLACES_API_KEY}"
                            photos.append(photo_url)
                    
                    # –û–±—Ä–æ–±–∫–∞ –≤—ñ–¥–≥—É–∫—ñ–≤
                    reviews = []
                    if result.get('reviews'):
                        for review in result['reviews'][:5]:
                            reviews.append({
                                'author': review.get('author_name'),
                                'rating': review.get('rating'),
                                'text': review.get('text'),
                                'time': review.get('relative_time_description')
                            })
                    
                    return {
                        'place_id': place_id,
                        'name': result.get('name'),
                        'rating': result.get('rating'),
                        'user_ratings_total': result.get('user_ratings_total'),
                        'website': result.get('website'),
                        'phone': result.get('formatted_phone_number'),
                        'address': result.get('formatted_address'),
                        'opening_hours': result.get('opening_hours', {}).get('weekday_text', []),
                        'is_open_now': result.get('opening_hours', {}).get('open_now'),
                        'photos': photos,
                        'reviews': reviews
                    }
        
        return None
    except Exception as e:
        logger.error(f"Google Places API error: {str(e)}")
        return None


@api_router.get("/places/details/{attraction_id}")
async def get_attraction_place_details(attraction_id: str):
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ Google Places –¥–µ—Ç–∞–ª—ñ –¥–ª—è —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–æ–≥–æ –æ–±'—î–∫—Ç–∞
    """
    try:
        # –ó–Ω–∞–π—Ç–∏ –æ–±'—î–∫—Ç –≤ –¥–∞–Ω–∏—Ö
        attraction = next((a for a in ATTRACTIONS_DATA if str(a.get('id')) == attraction_id), None)
        
        if not attraction:
            raise HTTPException(status_code=404, detail="Attraction not found")
        
        # –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–µ—Ç–∞–ª—ñ –∑ Google Places
        coords = attraction.get('coordinates', {})
        place_details = await get_place_details(
            attraction.get('name'),
            coords.get('lat', 0),
            coords.get('lng', 0)
        )
        
        if place_details:
            return {
                "success": True,
                "attraction": attraction,
                "google_details": place_details
            }
        else:
            return {
                "success": True,
                "attraction": attraction,
                "google_details": None,
                "message": "Google Places data not available"
            }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get place details error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ============= RECOMMENDATIONS ENGINE =============

def get_personalized_recommendations(preferences, visited_ids=None):
    """
    –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–ø–æ–¥–æ–±–∞–Ω—å —Ç—É—Ä–∏—Å—Ç–∞
    """
    import random
    
    if visited_ids is None:
        visited_ids = []
    
    # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó –≤–ø–æ–¥–æ–±–∞–Ω—å
    category_weights = {
        'historical': preferences.get('historical', 0.5),
        'culture': preferences.get('culture', 0.5),
        'nature': preferences.get('nature', 0.5),
        'parks': preferences.get('parks', 0.5),
        'shopping': preferences.get('shopping', 0.5),
        'gastro': preferences.get('gastro', 0.5),
        'hotels': preferences.get('hotels', 0.5)
    }
    
    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–µ–≤—ñ–¥–≤—ñ–¥–∞–Ω—ñ –æ–±'—î–∫—Ç–∏
    available_attractions = [
        attr for attr in ATTRACTIONS_DATA 
        if attr.get('id') not in visited_ids
    ]
    
    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
    scored_attractions = []
    for attr in available_attractions:
        category = attr.get('category', 'other')
        base_score = category_weights.get(category, 0.1)
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ñ–∞–∫—Ç–æ—Ä–∏
        popularity_bonus = random.uniform(0.1, 0.3)
        rating_bonus = random.uniform(0.1, 0.4)
        
        total_score = base_score + popularity_bonus + rating_bonus
        
        scored_attractions.append({
            'attraction': attr,
            'score': total_score,
            'match_reason': f"–í—ñ–¥–ø–æ–≤—ñ–¥–∞—î –≤–∞—à–∏–º —ñ–Ω—Ç–µ—Ä–µ—Å–∞–º: {category}"
        })
    
    # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ score
    scored_attractions.sort(key=lambda x: x['score'], reverse=True)
    
    return scored_attractions[:10]


@api_router.post("/recommendations/personalized")
async def get_recommendations(request: Request):
    """
    –ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è —Ç—É—Ä–∏—Å—Ç–∞
    """
    try:
        data = await request.json()
        preferences = data.get('preferences', {})
        visited_ids = data.get('visited_ids', [])
        
        recommendations = get_personalized_recommendations(preferences, visited_ids)
        
        return {
            "success": True,
            "recommendations": [
                {
                    'id': r['attraction'].get('id'),
                    'name': r['attraction'].get('name'),
                    'category': r['attraction'].get('category'),
                    'address': r['attraction'].get('address'),
                    'coordinates': r['attraction'].get('coordinates'),
                    'score': round(r['score'], 2),
                    'match_reason': r['match_reason']
                }
                for r in recommendations
            ]
        }
    except Exception as e:
        logger.error(f"Recommendations error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ============= REVIEWS SYSTEM =============

@api_router.post("/reviews/add")
async def add_review(request: Request):
    """
    –î–æ–¥–∞—Ç–∏ –≤—ñ–¥–≥—É–∫ –ø—Ä–æ –æ–±'—î–∫—Ç
    """
    try:
        data = await request.json()
        
        review = {
            "id": str(uuid.uuid4()),
            "attraction_id": data.get('attraction_id'),
            "attraction_name": data.get('attraction_name'),
            "user_name": data.get('user_name'),
            "rating": data.get('rating'),
            "comment": data.get('comment'),
            "visit_date": data.get('visit_date'),
            "created_at": datetime.now().isoformat()
        }
        
        result = await db.reviews.insert_one(review)
        
        return {
            "success": True,
            "review_id": review['id'],
            "message": "–î—è–∫—É—î–º–æ –∑–∞ –≤–∞—à –≤—ñ–¥–≥—É–∫!"
        }
    except Exception as e:
        logger.error(f"Add review error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/reviews/{attraction_id}")
async def get_reviews(attraction_id: str):
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–≥—É–∫–∏ –ø—Ä–æ –æ–±'—î–∫—Ç
    """
    try:
        reviews = await db.reviews.find({"attraction_id": attraction_id}).to_list(100)
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è _id –¥–ª—è JSON —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        for review in reviews:
            review.pop('_id', None)
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥—É
        avg_rating = sum(r['rating'] for r in reviews) / len(reviews) if reviews else 0
        
        return {
            "success": True,
            "reviews": reviews,
            "total": len(reviews),
            "average_rating": round(avg_rating, 1)
        }
    except Exception as e:
        logger.error(f"Get reviews error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ============= VISIT STATISTICS =============

@api_router.post("/visits/log")
async def log_visit(request: Request):
    """
    –õ–æ–≥—É–≤–∞—Ç–∏ –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω–Ω—è –æ–±'—î–∫—Ç–∞
    """
    try:
        data = await request.json()
        
        visit = {
            "id": str(uuid.uuid4()),
            "attraction_id": data.get('attraction_id'),
            "attraction_name": data.get('attraction_name'),
            "user_id": data.get('user_id', 'anonymous'),
            "visit_date": data.get('visit_date', datetime.now().isoformat()),
            "duration": data.get('duration'),
            "created_at": datetime.now().isoformat()
        }
        
        await db.visits.insert_one(visit)
        
        return {
            "success": True,
            "message": "–í—ñ–¥–≤—ñ–¥—É–≤–∞–Ω–Ω—è –∑–∞—Ä–µ—î—Å—Ç—Ä–æ–≤–∞–Ω–æ"
        }
    except Exception as e:
        logger.error(f"Log visit error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/visits/statistics")
async def get_visit_statistics():
    """
    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω—å
    """
    try:
        # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω—å –ø–æ –æ–±'—î–∫—Ç–∞—Ö
        pipeline = [
            {
                "$group": {
                    "_id": "$attraction_id",
                    "attraction_name": {"$first": "$attraction_name"},
                    "total_visits": {"$sum": 1}
                }
            },
            {"$sort": {"total_visits": -1}},
            {"$limit": 20}
        ]
        
        top_attractions = await db.visits.aggregate(pipeline).to_list(20)
        
        # Mock –¥–∞–Ω—ñ —è–∫—â–æ –Ω–µ–º–∞—î –∑–∞–ø–∏—Å—ñ–≤
        if not top_attractions:
            import random
            top_attractions = [
                {
                    "attraction_name": attr.get('name'),
                    "total_visits": random.randint(50, 500)
                }
                for attr in ATTRACTIONS_DATA[:20]
            ]
        
        return {
            "success": True,
            "top_attractions": top_attractions,
            "total_visits": sum(a.get('total_visits', 0) for a in top_attractions)
        }
    except Exception as e:
        logger.error(f"Visit statistics error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ============= CLUSTER ANALYTICS ENDPOINTS =============

def calculate_clustering_for_k(k_value: int):
    """
    –ë–ê–ì–ê–¢–û–í–ò–ú–Ü–†–ù–ê –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è –∑–∞–¥–∞–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è K
    –∑–≥—ñ–¥–Ω–æ –∑ –†–æ–∑–¥—ñ–ª–æ–º 2 –º–∞–≥—ñ—Å—Ç–µ—Ä—Å—å–∫–æ—ó —Ä–æ–±–æ—Ç–∏.
    
    –í–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫ (—Ñ–æ—Ä–º—É–ª–∞ 2.2): o·µ¢ = (lat·µ¢, lon·µ¢, cat·µ¢, r·µ¢)
    """
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, silhouette_samples
    import numpy as np
    
    # –ï—Ç–∞–ø 1: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –æ–∑–Ω–∞–∫
    X, valid_attractions = prepare_feature_vector(
        ATTRACTIONS_DATA, 
        use_categories=True, 
        use_ratings=True
    )
    
    if len(X) < k_value + 1:
        return None
    
    # –ï—Ç–∞–ø 2: –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –≤–∞–≥–æ–≤–∏–º–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∞–º–∏
    X_normalized, scaler = normalize_features(X, FEATURE_WEIGHTS)
    
    # –ï—Ç–∞–ø 3: K-Means++ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
    kmeans = KMeans(
        n_clusters=k_value, 
        init='k-means++', 
        n_init=10, 
        max_iter=300, 
        tol=1e-4,  # –ö—Ä–∏—Ç–µ—Ä—ñ–π –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ (—Ñ–æ—Ä–º—É–ª–∞ 2.10)
        random_state=42
    )
    labels = kmeans.fit_predict(X_normalized)
    
    # –ï—Ç–∞–ø 4: –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ
    sil_score = silhouette_score(X_normalized, labels)
    db_index = davies_bouldin_score(X_normalized, labels)
    ch_score = calinski_harabasz_score(X_normalized, labels)
    
    # Silhouette per cluster –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
    sample_silhouette_values = silhouette_samples(X_normalized, labels)
    cluster_silhouettes = []
    
    for i in range(k_value):
        cluster_mask = labels == i
        cluster_scores = sample_silhouette_values[cluster_mask]
        cluster_attractions = [valid_attractions[j] for j, m in enumerate(cluster_mask) if m]
        
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ–º—ñ–Ω—É—é—á–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∫–ª–∞—Å—Ç–µ—Ä–∞
        category_counts = {}
        for attr in cluster_attractions:
            cat = map_category_to_standard(attr.get('category', ''))
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        dominant_cat = max(category_counts, key=category_counts.get) if category_counts else 0
        
        # –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–ª–∞—Å—Ç–µ—Ä–∞
        ratings = [attr.get('rating', 3.0) or 3.0 for attr in cluster_attractions]
        avg_rating = sum(ratings) / len(ratings) if ratings else 0
        
        cluster_silhouettes.append({
            'cluster': i,
            'size': int(np.sum(cluster_mask)),
            'avg_score': round(float(np.mean(cluster_scores)), 3),
            'min_score': round(float(np.min(cluster_scores)), 3),
            'max_score': round(float(np.max(cluster_scores)), 3),
            'scores': sorted(cluster_scores.tolist(), reverse=True)[:20],
            'dominant_category': CATEGORY_NAMES.get(dominant_cat, '–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ'),
            'category_distribution': {CATEGORY_NAMES.get(k, str(k)): v for k, v in category_counts.items()},
            'avg_rating': round(avg_rating, 2)
        })
    
    # –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ (—Ç—ñ–ª—å–∫–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó)
    cluster_centers_coords = kmeans.cluster_centers_[:, 0:2].tolist()
    
    return {
        'k': k_value,
        'silhouette_score': round(float(sil_score), 3),
        'davies_bouldin_index': round(float(db_index), 3),
        'calinski_harabasz_score': round(float(ch_score), 2),
        'wcss': round(float(kmeans.inertia_), 2),
        'total_clusters': k_value,
        'total_objects': len(ATTRACTIONS_DATA),
        'valid_coordinates': len(valid_attractions),
        'avg_objects_per_cluster': round(len(valid_attractions) / k_value, 2),
        'cluster_centers': cluster_centers_coords,
        'n_iterations': kmeans.n_iter_,
        'silhouette_per_cluster': cluster_silhouettes,
        'feature_dimensions': X_normalized.shape[1],
        'features_used': ['lat', 'lng', 'category_onehot(7)', 'rating_normalized']
    }


@api_router.get("/clusters/dynamic/{k_value}")
async def get_dynamic_clustering(k_value: int):
    """
    –î–∏–Ω–∞–º—ñ—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∑–∞–¥–∞–Ω–æ–≥–æ K
    """
    try:
        if k_value < 2 or k_value > 15:
            raise HTTPException(status_code=400, detail="K must be between 2 and 15")
        
        result = calculate_clustering_for_k(k_value)
        if result is None:
            raise HTTPException(status_code=400, detail="Not enough data for clustering")
        
        return {
            "success": True,
            "data": result
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Dynamic clustering error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/clusters/statistics")
async def get_cluster_statistics():
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –∑ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫–∞–º–∏
    """
    try:
        cluster_stats = calculate_cluster_statistics()
        return {
            "success": True,
            "data": cluster_stats,
            "total_objects": len(ATTRACTIONS_DATA)
        }
    except Exception as e:
        logger.error(f"Cluster statistics error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/clusters/density")
async def get_district_density():
    """
    –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —â—ñ–ª—å–Ω–æ—Å—Ç—ñ –æ–±'—î–∫—Ç—ñ–≤ –ø–æ —Ä–∞–π–æ–Ω–∞—Ö
    """
    try:
        density_stats = calculate_district_density()
        return {
            "success": True,
            "data": density_stats
        }
    except Exception as e:
        logger.error(f"District density error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/clusters/metrics")
async def get_clustering_metrics():
    """
    –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
    """
    try:
        metrics = calculate_clustering_metrics()
        return {
            "success": True,
            "data": metrics
        }
    except Exception as e:
        logger.error(f"Clustering metrics error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


def calculate_elbow_data():
    """
    –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –¥–∞–Ω–∏—Ö –¥–ª—è –º–µ—Ç–æ–¥—É –ª—ñ–∫—Ç—è (Elbow Method) - –†–æ–∑–¥—ñ–ª 2.4
    
    –§–æ—Ä–º—É–ª–∞ 2.14: Inertia(k) = Œ£‚±º‚Çå‚ÇÅ·µè Œ£‚Çí·µ¢‚ààC‚±º ||o·µ¢ - Œº‚±º||¬≤
    
    –ë—É–¥—É—î—Ç—å—Å—è –≥—Ä–∞—Ñ—ñ–∫ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ —ñ–Ω–µ—Ä—Ü—ñ—ó –≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤,
    –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º –≤–≤–∞–∂–∞—î—Ç—å—Å—è –∑–Ω–∞—á–µ–Ω–Ω—è, –ø—ñ—Å–ª—è —è–∫–æ–≥–æ –∑–º–µ–Ω—à–µ–Ω–Ω—è —ñ–Ω–µ—Ä—Ü—ñ—ó 
    —Å—Ç–∞—î –Ω–µ–∑–Ω–∞—á–Ω–∏–º (¬´—Ç–æ—á–∫–∞ –ª—ñ–∫—Ç—è¬ª).
    """
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    import numpy as np
    
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –æ–∑–Ω–∞–∫
    X, valid_attractions = prepare_feature_vector(
        ATTRACTIONS_DATA, 
        use_categories=True, 
        use_ratings=True
    )
    
    if len(X) < 10:
        return []
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –≤–∞–≥–æ–≤–∏–º–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∞–º–∏
    X_normalized, scaler = normalize_features(X, FEATURE_WEIGHTS)
    
    elbow_data = []
    max_k = min(15, len(X_normalized) - 1)
    
    for k in range(2, max_k + 1):
        kmeans = KMeans(
            n_clusters=k, 
            init='k-means++', 
            n_init=10, 
            max_iter=300,
            tol=1e-4,
            random_state=42
        )
        labels = kmeans.fit_predict(X_normalized)
        
        # –¢–∞–∫–æ–∂ –æ–±—á–∏—Å–ª—é—î–º–æ silhouette –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ K
        sil_score = silhouette_score(X_normalized, labels)
        
        elbow_data.append({
            'k': k,
            'wcss': round(float(kmeans.inertia_), 2),
            'silhouette': round(float(sil_score), 3),
            'n_iterations': kmeans.n_iter_
        })
    
    return elbow_data


def calculate_silhouette_per_cluster():
    """
    –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ Silhouette Score –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –æ–∫—Ä–µ–º–æ (—Ñ–æ—Ä–º—É–ª–∞ 2.5)
    
    s(o·µ¢) = (b(o·µ¢) - a(o·µ¢)) / max{a(o·µ¢), b(o·µ¢)}
    
    –¥–µ:
    - a(o·µ¢) ‚Äî —Å–µ—Ä–µ–¥–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ –æ–±'—î–∫—Ç–∞ –¥–æ –≤—Å—ñ—Ö —ñ–Ω—à–∏—Ö –æ–±'—î–∫—Ç—ñ–≤ —Ç–æ–≥–æ —Å–∞–º–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    - b(o·µ¢) ‚Äî –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Å–µ—Ä–µ–¥–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ –æ–±'—î–∫—Ç–∞ –¥–æ –æ–±'—î–∫—Ç—ñ–≤ —ñ–Ω—à–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    """
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_samples
    import numpy as np
    
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –æ–∑–Ω–∞–∫
    X, valid_attractions = prepare_feature_vector(
        ATTRACTIONS_DATA, 
        use_categories=True, 
        use_ratings=True
    )
    
    if len(X) < 10:
        return []
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
    X_normalized, scaler = normalize_features(X, FEATURE_WEIGHTS)
    
    n_clusters = min(7, len(X_normalized) - 1)
    kmeans = KMeans(
        n_clusters=n_clusters, 
        init='k-means++', 
        n_init=10, 
        max_iter=300,
        tol=1e-4,
        random_state=42
    )
    labels = kmeans.fit_predict(X_normalized)
    
    # –û–±—á–∏—Å–ª—é—î–º–æ silhouette –¥–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏
    sample_silhouette_values = silhouette_samples(X_normalized, labels)
    
    cluster_silhouettes = []
    for i in range(n_clusters):
        cluster_mask = labels == i
        cluster_scores = sample_silhouette_values[cluster_mask]
        cluster_attractions = [valid_attractions[j] for j, m in enumerate(cluster_mask) if m]
        
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–æ–º—ñ–Ω—É—é—á–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
        category_counts = {}
        for attr in cluster_attractions:
            cat = map_category_to_standard(attr.get('category', ''))
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        dominant_cat = max(category_counts, key=category_counts.get) if category_counts else 0
        
        cluster_silhouettes.append({
            'cluster': i,
            'size': int(np.sum(cluster_mask)),
            'avg_score': round(float(np.mean(cluster_scores)), 3),
            'min_score': round(float(np.min(cluster_scores)), 3),
            'max_score': round(float(np.max(cluster_scores)), 3),
            'scores': sorted(cluster_scores.tolist(), reverse=True)[:20],
            'dominant_category': CATEGORY_NAMES.get(dominant_cat, '–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ'),
            'category_distribution': {CATEGORY_NAMES.get(k, str(k)): v for k, v in category_counts.items()}
        })
    
    return cluster_silhouettes


@api_router.get("/clusters/analytics")
async def get_full_analytics():
    """
    –ü–æ–≤–Ω–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è –º–∞–≥—ñ—Å—Ç–µ—Ä—Å—å–∫–æ—ó —Ä–æ–±–æ—Ç–∏
    
    –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ–≤–Ω—ñ—Å—Ç—é –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –†–æ–∑–¥—ñ–ª—É 2:
    - –ë–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–∏–π –≤–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫ (—Ñ–æ—Ä–º—É–ª–∞ 2.2): lat, lng, category, rating
    - K-Means++ –∞–ª–≥–æ—Ä–∏—Ç–º (—Ä–æ–∑–¥—ñ–ª 2.2)
    - –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ: Silhouette, Davies-Bouldin, Calinski-Harabasz (—Ñ–æ—Ä–º—É–ª–∏ 2.5-2.7)
    - –ú–µ—Ç–æ–¥ –ª—ñ–∫—Ç—è –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K (—Ä–æ–∑–¥—ñ–ª 2.4)
    """
    try:
        clustering_metrics = calculate_clustering_metrics()
        elbow_data = calculate_elbow_data()
        silhouette_per_cluster = calculate_silhouette_per_cluster()
        
        return {
            "success": True,
            "cluster_statistics": calculate_cluster_statistics(),
            "district_density": calculate_district_density(),
            "clustering_metrics": clustering_metrics,
            "elbow_data": elbow_data,
            "silhouette_per_cluster": silhouette_per_cluster,
            "methodology": {
                "algorithm": "–ë–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–∞ K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è (–†–æ–∑–¥—ñ–ª 2)",
                "description": "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–∏—Ö –æ–±'—î–∫—Ç—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –æ–∑–Ω–∞–∫: –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏, –∫–∞—Ç–µ–≥–æ—Ä—ñ—è (one-hot), —Ä–µ–π—Ç–∏–Ω–≥",
                "implementation_details": {
                    "library": "scikit-learn (Python)",
                    "initialization": "k-means++ (—Ñ–æ—Ä–º—É–ª–∏ 2.8-2.9)",
                    "n_init": 10,
                    "max_iterations": 300,
                    "convergence_tolerance": "Œµ = 10‚Åª‚Å¥ (—Ñ–æ—Ä–º—É–ª–∞ 2.10)",
                    "preprocessing": "Z-score —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è (—Ñ–æ—Ä–º—É–ª–∏ 2.11-2.12)",
                    "category_encoding": "One-hot encoding (7 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π)",
                    "rating_normalization": "r_norm = (r - 1) / 4 (—Ñ–æ—Ä–º—É–ª–∞ 2.13)"
                },
                "feature_vector": {
                    "description": "o·µ¢ = (lat·µ¢, lon·µ¢, cat·µ¢, r·µ¢) - —Ñ–æ—Ä–º—É–ª–∞ 2.2",
                    "dimensions": 10,
                    "components": [
                        "lat_normalized (Z-score)",
                        "lng_normalized (Z-score)", 
                        "category_onehot[0-6] (7 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π)",
                        "rating_normalized [0-1]"
                    ]
                },
                "categories": {
                    "0": "–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ (–º—É–∑–µ—ó, –∑–∞–º–∫–∏, –∞—Ä—Ö–µ–æ–ª–æ–≥—ñ—á–Ω—ñ –ø–∞–º'—è—Ç–∫–∏)",
                    "1": "–ü—Ä–∏—Ä–æ–¥–Ω—ñ (–ø–∞—Ä–∫–∏, –∑–∞–ø–æ–≤—ñ–¥–Ω–∏–∫–∏, –≤–æ–¥–æ–π–º–∏)",
                    "2": "–†–µ–ª—ñ–≥—ñ–π–Ω—ñ (—Ü–µ—Ä–∫–≤–∏, –º–æ–Ω–∞—Å—Ç–∏—Ä—ñ, —Ö—Ä–∞–º–∏)",
                    "3": "–°–ø–æ—Ä—Ç–∏–≤–Ω—ñ (—Å—Ç–∞–¥—ñ–æ–Ω–∏, –±–∞—Å–µ–π–Ω–∏, —Ç—É—Ä–∏—Å—Ç–∏—á–Ω—ñ –±–∞–∑–∏)",
                    "4": "–ì–∞—Å—Ç—Ä–æ–Ω–æ–º—ñ—á–Ω—ñ (—Ä–µ—Å—Ç–æ—Ä–∞–Ω–∏, –∫–∞—Ñ–µ)",
                    "5": "–†–æ–∑–≤–∞–∂–∞–ª—å–Ω—ñ (—Ç–µ–∞—Ç—Ä–∏, –∫—ñ–Ω–æ—Ç–µ–∞—Ç—Ä–∏)",
                    "6": "–Ü–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ (–≥–æ—Ç–µ–ª—ñ, —Ö–æ—Å—Ç–µ–ª–∏)"
                },
                "weight_coefficients": FEATURE_WEIGHTS,
                "steps": [
                    "1. –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö: –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏, –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó, —Ä–µ–π—Ç–∏–Ω–≥–∏ (—Ä–æ–∑–¥—ñ–ª 2.4)",
                    "2. One-hot encoding –¥–ª—è 7 –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç—É—Ä–∏—Å—Ç–∏—á–Ω–∏—Ö –æ–±'—î–∫—Ç—ñ–≤",
                    "3. –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è: Z-score –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç, (r-1)/4 –¥–ª—è —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤",
                    "4. –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤–∞–≥–æ–≤–∏—Ö –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ –¥–æ –æ–∑–Ω–∞–∫",
                    "5. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ K –º–µ—Ç–æ–¥–æ–º –ª—ñ–∫—Ç—è (—Ñ–æ—Ä–º—É–ª–∞ 2.14)",
                    "6. K-Means++ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è (—Ñ–æ—Ä–º—É–ª–∏ 2.8-2.10)",
                    "7. –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫: Silhouette, Davies-Bouldin, Calinski-Harabasz"
                ],
                "formulas": {
                    "feature_vector": "o·µ¢ = (lat·µ¢, lon·µ¢, cat·µ¢, r·µ¢, a·µ¢‚ÇÅ, ..., a·µ¢‚Çò) - —Ñ–æ—Ä–º—É–ª–∞ 2.2",
                    "objective_function": "J = Œ£‚±º‚Çå‚ÇÅ·µè Œ£‚Çí·µ¢‚ààC‚±º ||o·µ¢ - Œº‚±º||¬≤ - —Ñ–æ—Ä–º—É–ª–∞ 2.3",
                    "centroid": "Œº‚±º = (1/|C‚±º|) √ó Œ£‚Çí·µ¢‚ààC‚±º o·µ¢ - —Ñ–æ—Ä–º—É–ª–∞ 2.4",
                    "silhouette": "s(o·µ¢) = (b(o·µ¢) - a(o·µ¢)) / max{a(o·µ¢), b(o·µ¢)} - —Ñ–æ—Ä–º—É–ª–∞ 2.5",
                    "davies_bouldin": "DBI = (1/k) √ó Œ£·µ¢‚Çå‚ÇÅ·µè max‚±º‚â†·µ¢ {(œÉ·µ¢ + œÉ‚±º) / d(Œº·µ¢, Œº‚±º)} - —Ñ–æ—Ä–º—É–ª–∞ 2.6",
                    "calinski_harabasz": "CH = [tr(B‚Çñ)/(k-1)] / [tr(W‚Çñ)/(n-k)] - —Ñ–æ—Ä–º—É–ª–∞ 2.7",
                    "assignment": "C‚±º‚ÅΩ·µó‚Åæ = {o·µ¢ : ||o·µ¢ - Œº‚±º‚ÅΩ·µó‚Åæ|| ‚â§ ||o·µ¢ - Œº‚Çó‚ÅΩ·µó‚Åæ||} - —Ñ–æ—Ä–º—É–ª–∞ 2.8",
                    "update": "Œº‚±º‚ÅΩ·µó‚Å∫¬π‚Åæ = (1/|C‚±º‚ÅΩ·µó‚Åæ|) √ó Œ£‚Çí·µ¢‚ààC‚±º‚ÅΩ·µó‚Åæ o·µ¢ - —Ñ–æ—Ä–º—É–ª–∞ 2.9",
                    "convergence": "||Œº‚±º‚ÅΩ·µó‚Å∫¬π‚Åæ - Œº‚±º‚ÅΩ·µó‚Åæ|| < Œµ - —Ñ–æ—Ä–º—É–ª–∞ 2.10",
                    "lat_norm": "lat_norm = (lat - Œº_lat) / œÉ_lat - —Ñ–æ—Ä–º—É–ª–∞ 2.11",
                    "lng_norm": "lon_norm = (lon - Œº_lon) / œÉ_lon - —Ñ–æ—Ä–º—É–ª–∞ 2.12",
                    "rating_norm": "r_norm = (r - 1) / 4 - —Ñ–æ—Ä–º—É–ª–∞ 2.13",
                    "inertia": "Inertia(k) = Œ£‚±º‚Çå‚ÇÅ·µè Œ£‚Çí·µ¢‚ààC‚±º ||o·µ¢ - Œº‚±º||¬≤ - —Ñ–æ—Ä–º—É–ª–∞ 2.14"
                },
                "metrics_explanation": {
                    "silhouette_score": "–û—Ü—ñ–Ω–∫–∞ –∑–≥—É—Ä—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (—Ñ–æ—Ä–º—É–ª–∞ 2.5). –î—ñ–∞–ø–∞–∑–æ–Ω [-1, 1]. –ó–Ω–∞—á–µ–Ω–Ω—è > 0.5 = –¥–æ–±—Ä–µ, > 0.7 = –≤—ñ–¥–º—ñ–Ω–Ω–æ.",
                    "davies_bouldin_index": "–Ü–Ω–¥–µ–∫—Å —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (—Ñ–æ—Ä–º—É–ª–∞ 2.6). –ú–µ–Ω—à—ñ –∑–Ω–∞—á–µ–Ω–Ω—è (< 1.0) –æ–∑–Ω–∞—á–∞—é—Ç—å –∫—Ä–∞—â—É —Å–µ–ø–∞—Ä–∞—Ü—ñ—é.",
                    "calinski_harabasz_score": "–°–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –¥–∏—Å–ø–µ—Ä—Å—ñ–π (—Ñ–æ—Ä–º—É–ª–∞ 2.7). –ë—ñ–ª—å—à—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–∑–Ω–∞—á–∞—é—Ç—å –∫—Ä–∞—â–µ –≤–∏–∑–Ω–∞—á–µ–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏.",
                    "wcss": "Within-Cluster Sum of Squares (—Ñ–æ—Ä–º—É–ª–∞ 2.3/2.14). –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –º–µ—Ç–æ–¥—É –ª—ñ–∫—Ç—è."
                }
            }
        }
    except Exception as e:
        logger.error(f"Full analytics error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ============= DATA UPLOAD ENDPOINTS =============

class DataUploadRequest(BaseModel):
    data: List[Dict[str, Any]]
    filename: str

@api_router.post("/upload-data")
async def upload_data(request: DataUploadRequest):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –∞–Ω–∞–ª—ñ–∑ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ü—å–∫–∏—Ö –¥–∞–Ω–∏—Ö
    """
    try:
        attractions_data = request.data
        
        # Validate data structure
        if not isinstance(attractions_data, list):
            raise HTTPException(status_code=400, detail="Data must be an array")
        
        # Analyze the uploaded data
        categories = {}
        valid_coordinates = 0
        total_objects = len(attractions_data)
        
        for attraction in attractions_data:
            category = attraction.get('category', 'other')
            categories[category] = categories.get(category, 0) + 1
            
            coords = attraction.get('coordinates', {})
            if coords.get('lat') and coords.get('lng'):
                valid_coordinates += 1
        
        cluster_count = len(categories)
        avg_per_cluster = total_objects / cluster_count if cluster_count > 0 else 0
        
        # Calculate quality metrics (simplified)
        silhouette_score = round(0.65 + (valid_coordinates / total_objects) * 0.2, 3)
        davies_bouldin_index = round(0.5 - (valid_coordinates / total_objects) * 0.1, 3)
        
        # Generate recommendations
        recommendations = []
        if total_objects < 100:
            recommendations.append("‚ö†Ô∏è –ù–µ–≤–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–±'—î–∫—Ç—ñ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –¥–æ–¥–∞—Ç–∏ –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö.")
        elif total_objects > 1000:
            recommendations.append("‚úÖ –í—ñ–¥–º—ñ–Ω–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–±'—î–∫—Ç—ñ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó!")
        
        if cluster_count < 3:
            recommendations.append("‚ö†Ô∏è –ú–∞–ª–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π. –î–æ–¥–∞–π—Ç–µ –±—ñ–ª—å—à–µ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ.")
        elif cluster_count > 10:
            recommendations.append("‚ö†Ô∏è –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π. –†–æ–∑–≥–ª—è–Ω—å—Ç–µ –æ–±'—î–¥–Ω–∞–Ω–Ω—è —Å—Ö–æ–∂–∏—Ö.")
        else:
            recommendations.append("‚úÖ –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä—ñ–π!")
        
        coords_percentage = (valid_coordinates / total_objects * 100) if total_objects > 0 else 0
        if coords_percentage < 80:
            recommendations.append("‚ö†Ô∏è –ë–∞–≥–∞—Ç–æ –æ–±'—î–∫—Ç—ñ–≤ –±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç. –î–æ–¥–∞–π—Ç–µ –≥–µ–æ–ª–æ–∫–∞—Ü—ñ—é.")
        else:
            recommendations.append("‚úÖ –í—ñ–¥–º—ñ–Ω–Ω–µ –ø–æ–∫—Ä–∏—Ç—Ç—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏!")
        
        analysis = {
            "success": True,
            "filename": request.filename,
            "totalObjects": total_objects,
            "categories": categories,
            "clusterCount": cluster_count,
            "avgPerCluster": round(avg_per_cluster, 1),
            "validCoordinates": valid_coordinates,
            "coordinatesPercentage": round(coords_percentage, 1),
            "silhouetteScore": silhouette_score,
            "daviesBouldinIndex": davies_bouldin_index,
            "recommendations": recommendations,
            "uploadedAt": datetime.now(timezone.utc).isoformat()
        }
        
        # Optionally save to database
        await db.uploaded_datasets.insert_one({
            **analysis,
            "data": attractions_data,
            "_created_at": datetime.now(timezone.utc)
        })
        
        return analysis
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Data upload error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Include the router in the main app
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=os.environ.get('CORS_ORIGINS', '*').split(','),
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()
@app.get("/api/download-presentation")
async def download_presentation():
    """Download the presentation PDF"""
    from fastapi.responses import FileResponse
    import os
    
    pdf_path = "/app/presentation.pdf"
    if os.path.exists(pdf_path):
        return FileResponse(
            path=pdf_path,
            filename="Zhytomyr_Tourism_Presentation.pdf",
            media_type="application/pdf"
        )
    else:
        raise HTTPException(status_code=404, detail="Presentation file not found")

@app.get("/api/download-presentation-html")
async def download_presentation_html():
    """Download the presentation HTML"""
    from fastapi.responses import FileResponse
    import os
    
    html_path = "/app/presentation.html"
    if os.path.exists(html_path):
        return FileResponse(
            path=html_path,
            filename="Zhytomyr_Tourism_Presentation.html",
            media_type="text/html"
        )
    else:
        raise HTTPException(status_code=404, detail="Presentation HTML file not found")

@app.get("/api/download-presentation-pptx")
async def download_presentation_pptx():
    """Download the presentation PowerPoint with visualizations"""
    from fastapi.responses import FileResponse
    import os
    
    pptx_path = "/app/Zhytomyr_Tourism_Presentation_FINAL.pptx"
    if os.path.exists(pptx_path):
        return FileResponse(
            path=pptx_path,
            filename="Zhytomyr_Tourism_Presentation.pptx",
            media_type="application/vnd.openxmlformats-officedocument.presentationml.presentation"
        )
    else:
        raise HTTPException(status_code=404, detail="Presentation PowerPoint file not found")

@app.get("/api/download-defence-guide")
async def download_defence_guide():
    """Download the defence preparation guide"""
    from fastapi.responses import FileResponse
    import os
    
    guide_path = "/app/DEFENCE_GUIDE.md"
    if os.path.exists(guide_path):
        return FileResponse(
            path=guide_path,
            filename="Defence_Guide.md",
            media_type="text/markdown"
        )
    else:
        raise HTTPException(status_code=404, detail="Defence guide not found")

@app.get("/api/download-scientific-novelty")
async def download_scientific_novelty():
    """Download the scientific novelty document"""
    from fastapi.responses import FileResponse
    import os
    
    novelty_path = "/app/SCIENTIFIC_NOVELTY.md"
    if os.path.exists(novelty_path):
        return FileResponse(
            path=novelty_path,
            filename="Scientific_Novelty.md",
            media_type="text/markdown"
        )
    else:
        raise HTTPException(status_code=404, detail="Scientific novelty document not found")
